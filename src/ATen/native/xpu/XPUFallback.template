#include <ATen/core/Tensor.h>
#include <ATen/native/CPUFallback.h>

namespace at {

static bool DEBUG_XPU_FALLBACK = false;

static void xpu_fallback_impl(
    const c10::OperatorHandle& op,
    torch::jit::Stack* stack) {
  if (!DEBUG_XPU_FALLBACK) {
    TORCH_WARN_ONCE(
        "Aten Op fallback from XPU to CPU happends.",
        " This may have performance implications.",
        " If need debug the fallback ops please set environment variable `PYTORCH_DEBUG_XPU_FALLBACK=1` ");
  } else {
    TORCH_WARN(
        "The operator '",
        op.schema().operator_name(),
        " on the XPU backend is falling back to run on the CPU.");
  }
  native::cpu_fallback(op, stack, true);
}

namespace native::xpu {
Tensor nms(const Tensor& dets, const Tensor& scores, double iou_threshold_);
Tensor roi_align(const Tensor& input, const Tensor& rois, double spatial_scale, int64_t pooled_height, int64_t pooled_width, int64_t sampling_ratio, bool aligned);
Tensor _roi_align_backward(const Tensor& grad, const Tensor& rois, double spatial_scale, int64_t pooled_height, int64_t pooled_width, int64_t batch_size, int64_t channels, int64_t height, int64_t width, int64_t sampling_ratio, bool aligned);
std::tuple<Tensor, Tensor> ps_roi_align(const Tensor& input, const Tensor& rois, double spatial_scale, int64_t pooled_height, int64_t pooled_width, int64_t sampling_ratio);
Tensor _ps_roi_align_backward(const Tensor& grad, const Tensor& rois, const Tensor& channel_mapping, double spatial_scale, int64_t pooled_height, int64_t pooled_width, int64_t sampling_ratio, int64_t batch_size, int64_t channels, int64_t height, int64_t width);
std::tuple<Tensor, Tensor> roi_pool(const Tensor& input, const Tensor& rois, double spatial_scale, int64_t pooled_height, int64_t pooled_width);
Tensor _roi_pool_backward(const Tensor& grad, const Tensor& rois, const Tensor& argmax, double spatial_scale, int64_t pooled_height, int64_t pooled_width, int64_t batch_size, int64_t channels, int64_t height, int64_t width);
std::tuple<Tensor, Tensor> ps_roi_pool(const Tensor& input, const Tensor& rois, double spatial_scale, int64_t pooled_height, int64_t pooled_width);
Tensor _ps_roi_pool_backward(const Tensor& grad, const Tensor& rois, const Tensor& channel_mapping, double spatial_scale, int64_t pooled_height, int64_t pooled_width, int64_t batch_size, int64_t channels, int64_t height, int64_t width);
Tensor deform_conv2d(const Tensor& input, const Tensor& weight, const Tensor& offset, const Tensor& mask, const Tensor& bias, int64_t stride_h, int64_t stride_w, int64_t pad_h, int64_t pad_w, int64_t dilation_h, int64_t dilation_w, int64_t n_weight_grps, int64_t n_offset_grps, bool use_mask);
std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor> _deform_conv2d_backward( const Tensor& grad_out, const Tensor& input, const Tensor& weight, const Tensor& offset, const Tensor& mask, const Tensor& bias, int64_t stride_h, int64_t stride_w, int64_t pad_h, int64_t pad_w, int64_t dilation_h, int64_t dilation_w, int64_t n_weight_grps, int64_t n_offset_grps, bool use_mask);
}

// Register op's implementation lazily since sometimes the op is not defined,
// when registering implementation in PyTorch.

// Change both maps table and register_func when adding a new operator
// with lazy registration. So far, support torchvision namespace only.
// <operator_name: string, is_cpu_fallback: bool>
static std::map<std::string, bool> torchvision_ops_dispatching_table_ = {
  {"torchvision::nms", false},
  {"torchvision::roi_align", false},
  {"torchvision::_roi_align_backward", false},
  {"torchvision::ps_roi_align", false},
  {"torchvision::_ps_roi_align_backward", false},
  {"torchvision::roi_pool", false},
  {"torchvision::_roi_pool_backward", false},
  {"torchvision::ps_roi_pool", false},
  {"torchvision::_ps_roi_pool_backward", false},
  {"torchvision::deform_conv2d", false},
  {"torchvision::_deform_conv2d_backward", false},
};

// Return:
// true  - Redispatch to implementation lazily registered.
// false - Not redispatch.
static bool lazy_registration_and_redispatch(
    const c10::OperatorHandle& op,
    torch::jit::Stack* stack) {
  auto register_func =
      [](torch::Library& m) -> void {
        // Register all operators of torchvision namespace, not to register op
        // by op when the op is called. When a torchvision::op is called,
        // suppose ops of torchvision are all defined (`import torchvision`).
        m.impl(TORCH_SELECTIVE_NAME("torchvision::nms"), TORCH_FN(at::native::xpu::nms));
        m.impl(
            TORCH_SELECTIVE_NAME("torchvision::roi_align"),TORCH_FN(at::native::xpu::roi_align));
        m.impl(
            TORCH_SELECTIVE_NAME("torchvision::_roi_align_backward"),TORCH_FN(at::native::xpu::_roi_align_backward));
        m.impl(
            TORCH_SELECTIVE_NAME("torchvision::ps_roi_align"),TORCH_FN(at::native::xpu::ps_roi_align));
        m.impl(
            TORCH_SELECTIVE_NAME("torchvision::_ps_roi_align_backward"),TORCH_FN(at::native::xpu::_ps_roi_align_backward));
        m.impl(
            TORCH_SELECTIVE_NAME("torchvision::roi_pool"),TORCH_FN(at::native::xpu::roi_pool));
        m.impl(
            TORCH_SELECTIVE_NAME("torchvision::_roi_pool_backward"),TORCH_FN(at::native::xpu::_roi_pool_backward));
        m.impl(
            TORCH_SELECTIVE_NAME("torchvision::ps_roi_pool"),TORCH_FN(at::native::xpu::ps_roi_pool));
        m.impl(
            TORCH_SELECTIVE_NAME("torchvision::_ps_roi_pool_backward"),TORCH_FN(at::native::xpu::_ps_roi_pool_backward));
        m.impl(
            TORCH_SELECTIVE_NAME("torchvision::deform_conv2d"),TORCH_FN(at::native::xpu::deform_conv2d));
        m.impl(
            TORCH_SELECTIVE_NAME("torchvision::_deform_conv2d_backward"),TORCH_FN(at::native::xpu::_deform_conv2d_backward));
      };

  static const torch::detail::TorchLibraryInit
      torchvision_ops_impl_lazy_registration(
          torch::Library::IMPL,
          register_func,
          "torchvision",
          c10::make_optional(c10::DispatchKey::XPU),
          __FILE__,
          __LINE__);

  bool need_redispatch_after_lazy_registration =
      torchvision_ops_dispatching_table_.end() != torchvision_ops_dispatching_table_.find(op.schema().operator_name().name);
  bool is_cpu_fallback = need_redispatch_after_lazy_registration ?
      torchvision_ops_dispatching_table_[op.schema().operator_name().name] : true;

  if (need_redispatch_after_lazy_registration) {
    if (!is_cpu_fallback) {
      op.redispatchBoxed(c10::DispatchKeySet(c10::DispatchKey::XPU), stack);
    } else {
      xpu_fallback_impl(op, stack);
    }
    return true;
  } else {
    return false;
  }
}

static void xpu_fallback(
    const c10::OperatorHandle& op,
    torch::jit::Stack* stack) {
  if (lazy_registration_and_redispatch(op, stack)) {
    return;
  }

  // TODO: do Profiling if profiler.isCPUFallbackProfilingEnabled()
  xpu_fallback_impl(op, stack);
}

static void xpu_lazy_registration_or_error_fallback(
    const c10::OperatorHandle& op,
    torch::jit::Stack* stack) {
  if (!lazy_registration_and_redispatch(op, stack)) {
    TORCH_CHECK_NOT_IMPLEMENTED(
        false,
        "The operator '",
        op.schema().operator_name(),
        "' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. ",
        "You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. "
        "WARNING: this will bring unexpected performance compared with running natively on XPU.");
  }
}

TORCH_LIBRARY_IMPL(_, XPU, m) {
  static const char* enable_xpu_fallback =
      getenv("PYTORCH_ENABLE_XPU_FALLBACK");
  if (!enable_xpu_fallback || std::stoi(enable_xpu_fallback) == 0) {
    m.fallback(
        torch::CppFunction::makeFromBoxedFunction<&xpu_lazy_registration_or_error_fallback>());
  } else {
    m.fallback(torch::CppFunction::makeFromBoxedFunction<&xpu_fallback>());
  }

  static const char* debug_xpu_fallback = getenv("PYTORCH_DEBUG_XPU_FALLBACK");
  if (!debug_xpu_fallback || std::stoi(debug_xpu_fallback) == 0) {
    DEBUG_XPU_FALLBACK = false;
  } else {
    DEBUG_XPU_FALLBACK = true;
  }
}

/*
 * Register fallback to CPU for ops specified in env variable
 * "PYTORCH_XPU_FALLBACK_OP" , For example: export
 * PYTORCH_XPU_FALLBACK_OP=abs.out,div.Scalar,div.Tensor,div_.Scalar,div_.Tensor
 */
TORCH_LIBRARY_IMPL(aten, XPU, m) {
  static const char* fallback_op_str = getenv("PYTORCH_XPU_FALLBACK_OP");
  if (!fallback_op_str) {
    return;
  }
  std::istringstream iss(fallback_op_str);
  std::string op_name;
  while (std::getline(iss, op_name, ',')) {
    TORCH_WARN(
        "The operator '", op_name, "' registered to be forced to fallback to CPU.");
    m.impl(
        op_name.c_str(),
        torch::CppFunction::makeFromBoxedFunction<&xpu_fallback>());
  }
}

/*
 * These ops are not supported via XPU backend currently, and we fallback to run on CPU.
 */
TORCH_LIBRARY_IMPL(aten, XPU, m) {
  std::vector<std::string> fallback_list = {
    "cholesky",
    "cholesky_inverse",
    "_cholesky_solve_helper",
    "dot",
    "_efficient_attention_forward",
    "_flash_attention_forward",
    "geqrf",
    "linalg_cholesky_ex.L",
    "_linalg_det.result",
    "linalg_eig",
    "_linalg_eigh.eigenvalues",
    "linalg_householder_product",
    "linalg_inv_ex.inverse",
    "linalg_ldl_factor_ex.out",
    "linalg_ldl_solve.out",
    "linalg_lstsq.out",
    "linalg_lu_factor_ex.out",
    "linalg_lu.out",
    "linalg_lu_solve.out",
    "linalg_matrix_exp",
    "linalg_qr.out",
    "_linalg_slogdet.sign",
    "_linalg_solve_ex.result",
    "linalg_solve_triangular",
    "_linalg_svd.U",
    "lu_unpack.out",
    "ormqr",
    "_scaled_mm",
    "triangular_solve.X",
    "_validate_compressed_sparse_indices",
    "vdot",
  };
  for (auto& op_name : fallback_list) {
    m.impl(
        op_name.c_str(),
        torch::CppFunction::makeFromBoxedFunction<&xpu_fallback_impl>());
  }
}
TORCH_LIBRARY_IMPL(_inductor_test, XPU, m) {
    m.impl(
        "realize",
        torch::CppFunction::makeFromBoxedFunction<&xpu_fallback_impl>());
}
} // namespace at
