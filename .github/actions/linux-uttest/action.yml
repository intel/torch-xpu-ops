name: Linux Unit Test

inputs:
  ut_name:
    required: true
    type: string
    description: Which ut to launch
  keep_going:
    required: false
    type: string
    default: 'false'
    description: Define the Inductor UT test mechanism

permissions: read-all

runs:
  using: composite
  steps:
    - name: op_regression
      shell: timeout 3600 bash -xe {0}
      if: ${{ inputs.ut_name == 'op_regression' || inputs.ut_name == 'basic' }}
      run: |
        log_dir="${{ github.workspace }}/ut_log/${{ inputs.ut_name }}"
        mkdir -p ${log_dir}
        cd pytorch/third_party/torch-xpu-ops/test/regressions
        pytest --junit-xml=${{ github.workspace }}/ut_log/op_regression.xml \
          2> ${log_dir}/op_regression_test_error.log |tee ${log_dir}/op_regression_test.log
        echo -e "File Path: cd pytorch/third_party/torch-xpu-ops/test/regressions" | tee -a ${{ github.workspace }}/ut_log/reproduce_op_regression.log
        echo -e "Reproduce Command: pytest -sv failed_case" | tee -a ${{ github.workspace }}/ut_log/reproduce_op_regression.log
    - name: op_regression_dev1
      shell: timeout 300 bash -xe {0}
      if: ${{ inputs.ut_name == 'op_regression_dev1' || inputs.ut_name == 'basic' }}
      run: |
        log_dir="${{ github.workspace }}/ut_log/${{ inputs.ut_name }}"
        mkdir -p ${log_dir}
        cd pytorch/third_party/torch-xpu-ops/test/regressions
        timeout 180 pytest test_operation_on_device_1.py \
          --junit-xml=${{ github.workspace }}/ut_log/op_regression_dev1.xml \
          2> ${log_dir}/op_regression_dev1_test_error.log |tee ${log_dir}/op_regression_dev1_test.log
        echo -e "File Path: cd pytorch/third_party/torch-xpu-ops/test/regressions" | tee -a ${{ github.workspace }}/ut_log/reproduce_op_regression_dev1.log
        echo -e "Reproduce Command: pytest -sv failed_case" | tee -a ${{ github.workspace }}/ut_log/reproduce_op_regression_dev1.log
    - name: op_transformers
      shell: timeout 3600 bash -xe {0}
      if: ${{ inputs.ut_name == 'op_transformers' || inputs.ut_name == 'basic' }}
      run: |
        export PYTORCH_TEST_WITH_SLOW=1
        log_dir="${{ github.workspace }}/ut_log/${{ inputs.ut_name }}"
        mkdir -p ${log_dir}
        cd pytorch
        pytest test/test_transformers.py -k xpu \
          --junit-xml=${{ github.workspace }}/ut_log/op_transformers.xml \
          2> ${log_dir}/op_transformers_test_error.log |tee ${log_dir}/op_transformers_test.log
        echo -e "File Path: cd pytorch" | tee -a ${{ github.workspace }}/ut_log/reproduce_op_transformers.log
        echo -e "Reproduce Command: pytest -sv test/failed_case -k xpu" | tee -a ${{ github.workspace }}/ut_log/reproduce_op_transformers.log
    - name: op_extended
      shell: timeout 3600 bash -xe {0}
      if: ${{ inputs.ut_name == 'op_extended' || inputs.ut_name == 'basic' }}
      run: |
        export PYTORCH_TEST_WITH_SLOW=1
        log_dir="${{ github.workspace }}/ut_log/${{ inputs.ut_name }}"
        mkdir -p ${log_dir}
        cd pytorch/third_party/torch-xpu-ops/test/xpu/extended
        python run_test_with_skip.py \
          2> ${log_dir}/op_extended_test_error.log |tee ${log_dir}/op_extended_test.log
        ls -al
        cp *.xml ${{ github.workspace }}/ut_log
        echo -e "File Path: cd pytorch/third_party/torch-xpu-ops/test/xpu/extended" | tee -a ${{ github.workspace }}/ut_log/reproduce_op_extended.log
        echo -e "Reproduce Command: pytest -sv failed_case" | tee -a ${{ github.workspace }}/ut_log/reproduce_op_extended.log
    - name: op_ut
      shell: timeout 18000 bash -xe {0}
      if: ${{ inputs.ut_name == 'op_ut' }}
      run: |
        export PYTORCH_TEST_WITH_SLOW=1
        export PYTORCH_ENABLE_XPU_FALLBACK=1
        mkdir -p ut_log/op_ut
        cd pytorch/third_party/torch-xpu-ops/test/xpu
        python run_test_with_skip.py \
          2> ${{ github.workspace }}/ut_log/op_ut/op_ut_with_skip_test_error.log | \
          tee ${{ github.workspace }}/ut_log/op_ut/op_ut_with_skip_test.log
        ls -al
        cp *.xml ${{ github.workspace }}/ut_log
        find op_ut_with_skip_nn op_ut_with_skip_quantization/core -type f -exec sh -c '
            dir_path=$(dirname "$1");
            case "$dir_path" in
                *"op_ut_with_skip_quantization/core"*)
                    dir_name="op_ut_with_skip_quantization_core";;
                *)
                    dir_name=$(basename "$dir_path");;
            esac;
            mv "$1" "$dir_path/${dir_name}_$(basename "$1")"
        ' _ {} \;
        ls -al op_ut_with_skip_nn op_ut_with_skip_quantization/core
        cp op_ut_with_skip_nn/*.xml ${{ github.workspace }}/ut_log
        cp op_ut_with_skip_quantization/core/*.xml ${{ github.workspace }}/ut_log
        # Cases run with a on-demand white list, since some suites are too
        # slow to go through all operators on CPU. So add cases on-demand
        # when XPU implementatoin is done.
        # test_foreach, test_decomp
        # Run with only
        python run_test_with_only.py \
          2> ${{ github.workspace }}/ut_log/op_ut/op_ut_with_only_test_error.log | \
          tee ${{ github.workspace }}/ut_log/op_ut/op_ut_with_only_test.log
        ls -al
        cp *.xml ${{ github.workspace }}/ut_log
        echo -e "File Path: cd pytorch/third_party/torch-xpu-ops/test/xpu" | tee -a ${{ github.workspace }}/ut_log/reproduce_op_ut.log
        echo -e "Reproduce Command: pytest -sv failed_case" | tee -a ${{ github.workspace }}/ut_log/reproduce_op_ut.log
    - name: xpu_inductor
      shell: timeout 18000 bash -xe {0}
      if: ${{ inputs.ut_name == 'xpu_inductor' }}
      run: |
        export PYTORCH_TEST_WITH_SLOW=1
        export PYTORCH_TESTING_DEVICE_ONLY_FOR="xpu"
        keep_going_flag=""
        [[ "${{ inputs.keep_going }}" == "true" ]] && keep_going_flag="--keep-going"
        mkdir -p ut_log/xpu_inductor
        cd pytorch
        files=(
            "test_codecache.py"
            "test_kernel_benchmark.py"
            "test_max_autotune.py"
            "test_mkldnn_pattern_matcher.py"
            "test_triton_kernels.py"
            "test_torchinductor.py"
            "test_torchinductor_opinfo.py"
            "test_compile_subprocess.py"
            "test_compiled_optimizers.py"
            "test_compiled_autograd.py"
            "test_aot_inductor.py"
        )
        printf "%s\n" "${files[@]}" > test_files.txt
        cat test_files.txt
        for line in $(cat test_files.txt); do
          echo "=== Starting test: ${line} ==="
          start=$(date +%s)
          PYTEST_ADDOPTS="-v --timeout 600 --timeout_method=thread" NUM_PROCS=8 python test/run_test.py $keep_going_flag --xpu --include inductor/${line} 2>$GITHUB_WORKSPACE/ut_log/xpu_inductor/xpu_inductor_${line}_test_error.log | \
            tee $GITHUB_WORKSPACE/ut_log/xpu_inductor/xpu_inductor_${line}_test.log
          end=$(date +%s)
          echo -e "${line} duration: $((end - start))s"
          echo "=== Finished test: ${line} ==="
        done

        function read_dir(){
          for file in `ls $1`
          do
            if [ -d $1"/"$file ]
            then
              cp $1"/"$file"/"*.xml ${{ github.workspace }}/ut_log/
            else
              echo "[Warning] $file has no xml"
            fi
          done
        }
        read_dir "test/test-reports/python-pytest"
        function rename_inductor_files() {
          local target_dir="${1:-.}"
          if [ ! -d "$target_dir" ]; then
              echo "Error: Directory '$target_dir' does not exist" >&2
              return 1
          fi
          find "$target_dir" -name 'inductor.*' -exec bash -c '
              for file; do
                  newfile=$(echo "$file" | sed "s/inductor\./inductor_/")
                  if [ "$file" != "$newfile" ]; then
                      echo "Renaming: $file -> $newfile"
                      mv -v "$file" "$newfile"
                  fi
              done
          ' bash {} +
        }
        rename_inductor_files "${{ github.workspace }}/ut_log/"
    - name: xpu_profiling
      shell: timeout 3600 bash -xe {0}
      if: ${{ inputs.ut_name == 'xpu_profiling' }}
      run: |
        mkdir -p ut_log/xpu_profiling/issue_reproduce
        cd pytorch/third_party/torch-xpu-ops
        # RN50 Test
        PROFILE=1 python -u test/profiling/rn50.py -a resnet50 --dummy ./ --num-iterations 20 --xpu 0
        cp profiling.fp32.train.pt ${{ github.workspace }}/ut_log/xpu_profiling
        # All Issue Reproduce UT
        python -u test/profiling/correlation_id_mixed.py | \
          tee ${{ github.workspace }}/ut_log/xpu_profiling/issue_reproduce/correlation_id_mixed.log
        python -u test/profiling/reproducer.missing.gpu.kernel.time.py | \
          tee ${{ github.workspace }}/ut_log/xpu_profiling/issue_reproduce/reproducer.missing.gpu.kernel.time.log
        python -u test/profiling/time_precision_in_profile.py | \
          tee ${{ github.workspace }}/ut_log/xpu_profiling/issue_reproduce/time_precision_in_profile.log
        python -u test/profiling/profile_partial_runtime_ops.py | \
          tee ${{ github.workspace }}/ut_log/xpu_profiling/issue_reproduce/profile_partial_runtime_ops.log
        python -u test/profiling/triton_xpu_ops_time.py | \
          tee ${{ github.workspace }}/ut_log/xpu_profiling/issue_reproduce/triton_xpu_ops_time.log

        # llama case for calls number test
        pip install transformers
        python test/profiling/llama.py | \
          tee ${{ github.workspace }}/ut_log/xpu_profiling/llama.log
        python .github/scripts/llama_summary.py -i ${{ github.workspace }}/ut_log/xpu_profiling/llama.log -o ${{ github.workspace }}/ut_log/xpu_profiling/llama_summary.csv
        bash .github/scripts/check_baseline.sh .github/scripts/llama_baseline.csv ${{ github.workspace }}/ut_log/xpu_profiling/llama_summary.csv

        # All xpu ut under test/profiler
        cd ../../test/profiler
        python -m pytest -s test_cpp_thread.py | \
          tee ${{ github.workspace }}/ut_log/xpu_profiling/test_cpp_thread.log
        python -m pytest -s test_execution_trace.py | \
          tee ${{ github.workspace }}/ut_log/xpu_profiling/test_execution_trace.log
        python -m pytest -s test_memory_profiler.py | \
          tee ${{ github.workspace }}/ut_log/xpu_profiling/test_memory_profiler.log
        python -m pytest -s -vs test_profiler_tree.py | \
          tee ${{ github.workspace }}/ut_log/xpu_profiling/test_profiler_tree.log

    - name: xpu_distributed
      shell: timeout 3600 bash -xeu -o pipefail {0}
      if: ${{ inputs.ut_name == 'xpu_distributed' }}
      run: |
        xpu-smi topology -m
        sudo rm -rf ${{ github.workspace }}/ptrace_scope.bk
        sudo cp /proc/sys/kernel/yama/ptrace_scope ${{ github.workspace }}/ptrace_scope.bk
        cat ${{ github.workspace }}/ptrace_scope.bk
        echo "0" |sudo tee /proc/sys/kernel/yama/ptrace_scope
        mkdir -p ut_log/xpu_distributed
        cd pytorch/third_party/torch-xpu-ops/test/xpu
        XCCL_ENABLE=$(python -c "import torch;print(torch.distributed.is_xccl_available())")
        if [[ "${XCCL_ENABLE,,}" == 'false' ]] || [[ "${XCCL_ENABLE}" == '0' ]]; then
          echo -e "[ERROR] XCCL is not enabled"
          exit 1
        fi
        python run_distributed.py \
          2> ${{ github.workspace }}/ut_log/xpu_distributed/xpu_distributed_test_error.log | \
          tee ${{ github.workspace }}/ut_log/xpu_distributed/xpu_distributed_test.log
