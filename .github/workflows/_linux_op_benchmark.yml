name: Linux OP Benchmark Test

on:
  workflow_call:
    inputs:
      pytorch:
        required: false
        type: string
        default: 'main'
        description: Pytorch branch/commit
      keep_torch_xpu_ops:
        required: false
        type: string
        default: 'false'
        description: Keep torch-xpu-ops pin. `true` means use pined commit
      triton:
        required: false
        type: string
        default: ''
        description: Triton commit. Use pytorch pined commit by default
      python:
        required: false
        type: string
        default: '3.10'
        description: Python version
      runner:
        required: true
        type: string
        default: 'linux.idc.xpu'
        description: Runner label
      driver:
        required: false
        type: string
        default: 'rolling'
        description: Driver lts/rolling

permissions: read-all

jobs:
  op_benchmark_test:
    runs-on: ${{ inputs.runner }} 
    timeout-minutes: 900
    env:
      GH_TOKEN: ${{ github.token }}
      NEOReadDebugKeys: ${{ inputs.driver == 'rolling' && '1' || '0' }}
      DisableScratchPages: ${{ inputs.driver == 'rolling' && '1' || '0' }}
    steps:
      - name: Checkout torch-xpu-ops
        uses: actions/checkout@v4
      - name: Prepare Stock Pytorch
        run: |
          pwd
          which conda && conda clean -ay
          conda remove --all -y -n xpu_op_${ZE_AFFINITY_MASK} || \
                rm -rf $(dirname ${CONDA_EXE})/../envs/xpu_op_${ZE_AFFINITY_MASK}
          conda create -n xpu_op_${ZE_AFFINITY_MASK} python=${{ inputs.python }} cmake ninja -y
          source activate xpu_op_${ZE_AFFINITY_MASK}
          cd ../ && rm -rf pytorch
          pip install requests
          git clone https://github.com/pytorch/pytorch pytorch
          if [ "${{ inputs.pytorch }}" != "nightly_wheel" ]; then
            cd pytorch && git checkout $(echo ${{ inputs.pytorch }})
            # apply PRs for stock pytorch
            python ../torch-xpu-ops/.github/scripts/apply_torch_pr.py
            git status && git show -s
            git submodule sync && git submodule update --init --recursive
            if [[ ${{ inputs.keep_torch_xpu_ops }} == 'true' ]]; then
              echo "Don't replace torch-xpu-ops!"
            else
              rm -rf third_party/torch-xpu-ops && cp -r ../torch-xpu-ops third_party/
              # Workaround for torch-xpu-ops ci test
              sed -i "s/checkout --quiet \${TORCH_XPU_OPS_COMMIT}/log -n 1/g" caffe2/CMakeLists.txt
            fi
          fi
      - name: Download Pytorch wheel
        if: ${{ inputs.pytorch != 'nightly_wheel' }}
        uses: actions/download-artifact@v4
        with:
          name: Torch-XPU-Wheel-${{ github.event.pull_request.number || github.sha }}
          path: ${{ github.workspace }}
      - name: Install Pytorch XPU
        run: |
          source activate xpu_op_${ZE_AFFINITY_MASK}
          source .github/scripts/env.sh ${{ inputs.pytorch }}
          if [ "${{ inputs.pytorch }}" != "nightly_wheel" ]; then
            cd ../pytorch
            export CMAKE_PREFIX_PATH=${CMAKE_PREFIX_PATH}:${CONDA_PREFIX:-"$(dirname $(which conda))/../"}
            pip install -r requirements.txt
            pip install --force-reinstall ${{ github.workspace }}/torch*.whl
            git clone https://github.com/pytorch/vision && cd vision && python setup.py install && cd ..
          else
            pip install torch torchvision torchaudio --pre --index-url https://download.pytorch.org/whl/nightly/xpu
            TORCH_COMMIT_ID=$(python -c 'import torch; print(torch.version.git_version)')
          fi
          pip install -r .ci/docker/requirements-ci.txt
      - name: Torch Config
        run: |
          source activate xpu_op_${ZE_AFFINITY_MASK}
          source .github/scripts/env.sh ${{ inputs.pytorch }}
          python -c "import torch; print(torch.__config__.show())"
          python -c "import torch; print(torch.__config__.parallel_info())"
          python -c "import torch; print(torch.__config__.torch.xpu.device_count())"

          cd ..
          python pytorch/torch/utils/collect_env.py
          rm -rf /tmp/torchinductor_*
          rm -rf ~/.triton/cache
      - name: Run Torch XPU Op Benchmark
        if: ${{ inputs.driver == 'rolling' }} 
        run: |
          source .github/scripts/env.sh ${{ inputs.pytorch }}
          source activate xpu_op_${ZE_AFFINITY_MASK}
          mkdir -p ${{ github.workspace }}/op_benchmark
          cd test/microbench
          filename=$(find -- *.py)
          for i in $filename
          do
            python ${i%.*}.py > ${{ github.workspace }}/op_benchmark/${i%.*}.log
          done
          # Summary forward op time
          bash ${{ github.workspace }}/.github/scripts/microbench_summary.sh ${{ github.workspace }}/op_benchmark ${{ github.workspace }}/op_benchmark/forward_op_summary.csv
          # Summary backward op time
          bash ${{ github.workspace }}/.github/scripts/microbench_summary.sh ${{ github.workspace }}/op_benchmark ${{ github.workspace }}/op_benchmark/backward_op_summary.csv True
      - name: Download OP Baseline
        continue-on-error: true
        run: |
          gh api \
            --method GET -F per_page=100 -F page=10 \
            -H "Accept: application/vnd.github+json" -H "X-GitHub-Api-Version: 2022-11-28" \
            /repos/${GITHUB_REPOSITORY:-"intel/torch-xpu-ops"}/actions/artifacts \
            > ${GITHUB_WORKSPACE:-"/tmp"}/refer.json
          artifact_id="$(eval "jq -r \
                  '[.artifacts[] | \
                  select(.name|test(\"Inductor-XPU-OP-Benchmark-Data.*\")) | \
                  select(.workflow_run.head_branch|test(\"main\"))][0].id' \
              ${GITHUB_WORKSPACE:-"/tmp"}/refer.json")"
          if [ "$artifact_id" -gt 1 ];then
              gh api \
                  -H "Accept: application/vnd.github+json" \
                  -H "X-GitHub-Api-Version: 2022-11-28" \
                  /repos/${GITHUB_REPOSITORY:-"intel/torch-xpu-ops"}/actions/artifacts/${artifact_id}/zip > reference.zip
          fi
          rm -rf ${GITHUB_WORKSPACE:-"/tmp"}/reference
          mkdir ${GITHUB_WORKSPACE:-"/tmp"}/reference
          mv reference.zip ${GITHUB_WORKSPACE:-"/tmp"}/reference
          unzip ${GITHUB_WORKSPACE:-"/tmp"}/reference/reference.zip -d ${GITHUB_WORKSPACE:-"/tmp"}/reference > /dev/null 2>&1

          mkdir ${{ github.workspace }}/baseline
          if [ -f "${GITHUB_WORKSPACE:-"/tmp"}/reference/new_baseline/baseline_forward_op_summary.csv" ]; then
            cp ${GITHUB_WORKSPACE:-"/tmp"}/reference/new_baseline/baseline_forward_op_summary.csv ${{ github.workspace }}/baseline
            cp ${GITHUB_WORKSPACE:-"/tmp"}/reference/new_baseline/baseline_backward_op_summary.csv ${{ github.workspace }}/baseline
          else
            cp ${GITHUB_WORKSPACE:-"/tmp"}/reference/forward_op_summary.csv ${{ github.workspace }}/baseline/baseline_forward_op_summary.csv
            cp ${GITHUB_WORKSPACE:-"/tmp"}/reference/backward_op_summary.csv ${{ github.workspace }}/baseline/baseline_backward_op_summary.csv
          fi
      - name: Check OP Baseline
        run: |
          forward_exists=$([ -f "baseline/baseline_forward_op_summary.csv" ] && echo "true" || echo "false")
          backward_exists=$([ -f "baseline/baseline_backward_op_summary.csv" ] && echo "true" || echo "false")
          echo "forward_exists=${forward_exists}" >> $GITHUB_OUTPUT
          echo "backward_exists=${backward_exists}" >> $GITHUB_OUTPUT
          echo "both_exist=$([ "$forward_exists" = "true" ] && [ "$backward_exists" = "true" ] && echo "true" || echo "false")" >> $GITHUB_OUTPUT
      - name: Initial the OP Baseline
        if: steps.check-baseline.outputs.both_exist != 'true'
        run: |
          mkdir ${{ github.workspace }}/new_baseline
          cp ${{ github.workspace }}/op_benchmark/forward_op_summary.csv ${{ github.workspace }}/new_baseline/baseline_forward_op_summary.csv
          cp ${{ github.workspace }}/op_benchmark/backward_op_summary.csv ${{ github.workspace }}/new_baseline/baseline_backward_op_summary.csv
          cp -r ${{ github.workspace }}/new_baseline ${{ github.workspace }}/op_benchmark
          echo "Initial the Baseline Data"
      - name: Check the OP Regression
        if: steps.check-baseline.outputs.both_exist == 'true'
        run: |
          source .github/scripts/env.sh ${{ inputs.pytorch }}
          source activate xpu_op_${ZE_AFFINITY_MASK}
          pip install tabulate
          # Compare forward op
          python ${{ github.workspace }}/.github/scripts/op_perf_comparison.py --xpu_file ${{ github.workspace }}/op_benchmark/forward_op_summary.csv --baseline_file ${{ github.workspace }}/baseline/baseline_forward_op_summary.csv
          # Compare backward op
          python ${{ github.workspace }}/.github/scripts/op_perf_comparison.py --xpu_file ${{ github.workspace }}/op_benchmark/backward_op_summary.csv --baseline_file ${{ github.workspace }}/baseline/baseline_backward_op_summary.csv
      - name: Update OP Baseline
        if: steps.check-baseline.outputs.both_exist == 'true'
        run: |
          source .github/scripts/env.sh ${{ inputs.pytorch }}
          source activate xpu_op_${ZE_AFFINITY_MASK}
          mkdir ${{ github.workspace }}/new_baseline
          # Update forward op
          python ${{ github.workspace }}/.github/scripts/op_calculate_best_perf.py --xpu ${{ github.workspace }}/op_benchmark/forward_op_summary.csv --baseline ${{ github.workspace }}/new_baseline/baseline_forward_op_summary.csv -r
          # Update backward op
          python ${{ github.workspace }}/.github/scripts/op_calculate_best_perf.py --xpu ${{ github.workspace }}/op_benchmark/backward_op_summary.csv --baseline ${{ github.workspace }}/new_baseline/baseline_backward_op_summary.csv -r
          cp -r ${{ github.workspace }}/new_baseline ${{ github.workspace }}/op_benchmark
      - name: Upload Inductor XPU OP benchmark Log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: Inductor-XPU-OP-Benchmark-Data-${{ github.event.pull_request.number || github.sha }}
          path: ${{ github.workspace }}/op_benchmark
