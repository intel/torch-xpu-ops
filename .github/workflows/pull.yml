name: pull

on:
  pull_request:
    branches:
      - main
      - release/*

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.sha }}
  cancel-in-progress: true

permissions: read-all

jobs:
  preci-ut:
    # Don't run on forked repos
    if: github.repository_owner == 'intel'
    name: preci-ut
    runs-on: linux.idc.xpu
    timeout-minutes: 240
    steps:
      - name: Checkout torch-xpu-ops
        uses: actions/checkout@v3
      - name: Prepare Stock Pytorch
        run: |
          pwd
          cd ../ && rm -rf pytorch
          git clone https://github.com/pytorch/pytorch
          cd pytorch
          git checkout release/2.4
          git log -n 1 && git submodule sync && git submodule update --init --recursive
      - name: Build Pytorch XPU
        run: |
          which conda && conda clean -ay
          conda remove --all -y -n xpu_op_${ZE_AFFINITY_MASK} || \
                rm -rf $(dirname ${CONDA_EXE})/../envs/xpu_op_${ZE_AFFINITY_MASK}
          conda create -n xpu_op_${ZE_AFFINITY_MASK} python=3.10 cmake ninja -y
          source activate xpu_op_${ZE_AFFINITY_MASK}
          conda install -c intel mkl-static mkl-include -y
          cd ../pytorch
          pip install -r requirements.txt
          export USE_XPU=1
          source /opt/intel/oneapi/compiler/latest/env/vars.sh
          export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}
          python setup.py bdist_wheel
          pip install --force-reinstall dist/*.whl
          TORCHVISION_COMMIT_ID="$(cat .github/ci_commit_pins/vision.txt)"
          git clone https://github.com/pytorch/vision
          cd vision
          git checkout ${TORCHVISION_COMMIT_ID}
          python setup.py install && cd ..
          pip install -r .ci/docker/requirements-ci.txt
      - name: Run XPU OP Examples
        if: ${{ hashFiles('examples/') != '' }}
        run: |
          xpu-smi discovery
          source /opt/intel/oneapi/compiler/latest/env/vars.sh
          source activate xpu_op_${ZE_AFFINITY_MASK}
          cd examples
          pip install pytest
          timeout 8000 pytest -v
      - name: Run XPU OP UT Finegrain
        if: ${{ hashFiles('test/xpu/') != '' }}
        run: |
          source /opt/intel/oneapi/compiler/latest/env/vars.sh
          source activate xpu_op_${ZE_AFFINITY_MASK}
          export PYTORCH_TEST_WITH_SLOW=1
          cd ../pytorch/third_party/torch-xpu-ops/test/xpu/fin_grain
          timeout 10000 python run_fine_grain.py
      - name: Run XPU OP UT
        if: ${{ hashFiles('test/xpu/') != '' }}
        run: |
          source /opt/intel/oneapi/compiler/latest/env/vars.sh
          source activate xpu_op_${ZE_AFFINITY_MASK}
          export PYTORCH_ENABLE_XPU_FALLBACK=1
          export PYTORCH_TEST_WITH_SLOW=1
          cd ../pytorch/third_party/torch-xpu-ops/test/xpu
          timeout 10000 python run_test_with_skip.py || true
          # Cases run with a on-demand white list, since some suites are too
          # slow to go through all operators on CPU. So add cases on-demand
          # when XPU implementatoin is done.
          # test_foreach, test_decomp
          timeout 10000 python run_test_with_only.py || true
      - name: Run Torch XPU UT
        run: |
          source /opt/intel/oneapi/compiler/latest/env/vars.sh
          source activate xpu_op_${ZE_AFFINITY_MASK}
          cd ../pytorch
          TEST_REPORTS_DIR=$(pwd)/test/test-reports
          rm -rf "$TEST_REPORTS_DIR" && mkdir -p "$TEST_REPORTS_DIR"
          # Run Pytorch XPU binary UT
          for xpu_case in build/bin/*{xpu,sycl}*; do
            if [[ "$xpu_case" != *"*"* && "$xpu_case" != *.so && "$xpu_case" != *.a ]]; then
              case_name=$(basename "$xpu_case")
              echo "Testing ${case_name} ..."
              "$xpu_case" --gtest_output=xml:"$TEST_REPORTS_DIR"/"$case_name".xml
            fi
          done
          # Run Pytorch XPU python UT
          export PYTORCH_ENABLE_XPU_FALLBACK=1
          sed -i 's/selected_tests = exclude_tests(XPU_BLOCKLIST.*/selected_tests = XPU_TEST/g' ./test/run_test.py
          python test/run_test.py --xpu
